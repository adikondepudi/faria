{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "data.rename(columns={\"cerebellum_L\": \"cerebellumL\", \"cerebellum_R\": \"cerebellumR\", \"thalamus_L\": \"ThalamusL\", \"thalamus_R\": \"ThalamusR\", \"ponsmedulla\": \"pons\"}, inplace=True)\n",
    "data = data.sort_index(axis=1)\n",
    "output = pd.read_csv('output.csv', index_col=0)\n",
    "output = output.fillna(0)\n",
    "output = output.replace(\"x\", 1)\n",
    "output = output.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[output.index.isin(data.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, output, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "# Since we might have multiple columns (parts of the brain) to predict for, we'll need to fit separate models\n",
    "# For simplicity, let's assume we're building a separate model for each part of the brain\n",
    "models = {}\n",
    "for column in y_train.columns:\n",
    "    # Train a model for each part of the brain\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train_scaled, y_train[column])\n",
    "    models[column] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating salvageability for PCAL:\n",
      "Accuracy for PCAL: 0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.73         7\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.62      0.79      0.56         8\n",
      "weighted avg       0.91      0.62      0.69         8\n",
      "\n",
      "Evaluating salvageability for PCAR:\n",
      "Accuracy for PCAR: 0.38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.33      0.44         6\n",
      "           1       0.20      0.50      0.29         2\n",
      "\n",
      "    accuracy                           0.38         8\n",
      "   macro avg       0.43      0.42      0.37         8\n",
      "weighted avg       0.55      0.38      0.40         8\n",
      "\n",
      "Evaluating salvageability for ThalamusL:\n",
      "Accuracy for ThalamusL: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n",
      "Evaluating salvageability for ThalamusR:\n",
      "Accuracy for ThalamusR: 0.38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.38         8\n",
      "   macro avg       0.30      0.25      0.27         8\n",
      "weighted avg       0.45      0.38      0.41         8\n",
      "\n",
      "Evaluating salvageability for cerebellumL:\n",
      "Accuracy for cerebellumL: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83         6\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.67      0.67      0.67         8\n",
      "weighted avg       0.75      0.75      0.75         8\n",
      "\n",
      "Evaluating salvageability for cerebellumR:\n",
      "Accuracy for cerebellumR: 0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.57      0.58      0.56         8\n",
      "weighted avg       0.68      0.62      0.65         8\n",
      "\n",
      "Evaluating salvageability for midbrain:\n",
      "Accuracy for midbrain: 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88         8\n",
      "   macro avg       0.50      0.44      0.47         8\n",
      "weighted avg       1.00      0.88      0.93         8\n",
      "\n",
      "Evaluating salvageability for pons:\n",
      "Accuracy for pons: 0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.36      0.42      0.38         8\n",
      "weighted avg       0.54      0.62      0.58         8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adikondepudi/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adikondepudi/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adikondepudi/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Now predict for each part and store the results\n",
    "predictions = {}\n",
    "for column in models:\n",
    "    # Predict using the model trained for this part of the brain\n",
    "    predictions[column] = models[column].predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to a DataFrame for easy comparison\n",
    "predictions_df = pd.DataFrame(predictions, index=X_test.index)\n",
    "\n",
    "# Compare predictions with the true output\n",
    "for column in predictions_df.columns:\n",
    "    print(f\"Evaluating salvageability for {column}:\")\n",
    "    accuracy = accuracy_score(y_test[column], predictions_df[column])\n",
    "    print(f\"Accuracy for {column}: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test[column], predictions_df[column]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
